{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('traindataset.csv',memory_map=True) #importing cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"Housing Situation\",\"Satisfation with employer\",\n",
    "                    \"Gender\",\"Country\",\"Profession\",\"University Degree\",\"Hair Color\"]\n",
    "conti_cols = [\"Year of Record\",\"Crime Level in the City of Employement\",\n",
    "              \"Work Experience in Current Job [years]\",\"Age\",\"Size of City\",\"Wears Glasses\",\n",
    "              \"Body Height [cm]\",\"Yearly Income in addition to Salary (e.g. Rental Income)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes a string column name and returns a list\n",
    "#containing indices of dataframe that have outliers in that column\n",
    "#Refer: https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623\n",
    "def OutlierByColumn(colname):\n",
    "    rows = data.shape[0]\n",
    "    col_std = np.std(data[colname])\n",
    "    col_mean = np.mean(data[colname])\n",
    "    anomaly_cut_off = col_std * 3\n",
    "    lower_limit  = col_mean - anomaly_cut_off \n",
    "    upper_limit = col_mean + anomaly_cut_off\n",
    "    anomalies_indices = []\n",
    "    for i in range(rows):\n",
    "        ele = data[colname][i]\n",
    "        if ele > upper_limit or ele < lower_limit:\n",
    "            anomalies_indices.append(i)\n",
    "    return anomalies_indices        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991712, 18)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #checking no of rows and no of columns for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting indices of outliers for continuous labels\n",
    "#This will take some time to execute\n",
    "arr1 = OutlierByColumn('Work Experience in Current Job [years]')\n",
    "arr2 = OutlierByColumn('Age')\n",
    "arr3 = OutlierByColumn('Body Height [cm]')\n",
    "arr4 = OutlierByColumn('Yearly Income in addition to Salary (e.g. Rental Income)')\n",
    "arr5 = OutlierByColumn('Size of City')\n",
    "arr6 = OutlierByColumn('Total Yearly Income [EUR]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70317"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Union of all lists \n",
    "union_index = np.union1d(arr1,arr2)\n",
    "union_index = np.union1d(union_index,arr3)\n",
    "union_index = np.union1d(union_index,arr4)\n",
    "union_index = np.union1d(union_index,arr5)\n",
    "union_index = np.union1d(union_index,arr6)\n",
    "len(union_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read somewhere that binary encoders take less space\n",
    "import category_encoders as ce\n",
    "encoder = ce.BinaryEncoder(categorical_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(union_index) #removing all columns containing one or more outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding categorical columns\n",
    "encoded = encoder.fit_transform(data[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Total Yearly Income [EUR]'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining encoded columns with continous columns\n",
    "encoded = encoded.join(data[conti_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921395, 49)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encoded.values \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_regression\\nfrom sklearn.feature_selection import chi2\\nskb = SelectKBest(f_regression, k=47)         HERE K DENOTES NUMBER OF FEATURES I TRIED FROM 10 TILL 47, \\n                                              THE MODEL GOT BETTER WITH MORE FEATURES, THE BEST WHEN ALL ARE THERE\\nskb.fit(x, y)\\nx = skb.transform(x)'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I was trying to select K-best features but, as it turns out, it is best to include all features. \n",
    "'''from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import chi2\n",
    "skb = SelectKBest(f_regression, k=47)         HERE K DENOTES NUMBER OF FEATURES I TRIED FROM 10 TILL 47, \n",
    "                                              THE MODEL GOT BETTER WITH MORE FEATURES, THE BEST WHEN ALL ARE THERE\n",
    "skb.fit(x, y)\n",
    "x = skb.transform(x)''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577\n",
    "from sklearn import preprocessing\n",
    "scaler_model = preprocessing.StandardScaler().fit(x)\n",
    "x = scaler_model.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating data into train and test\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34851.65046822184"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ExtraTreesRegressor\n",
    "#This will take some time to execute \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "gbr = ExtraTreesRegressor(n_estimators=25,max_features=\"sqrt\")\n",
    "gbr.fit(xtrain.astype(int),ytrain.astype(int))\n",
    "ypred = gbr.predict(xtest.astype(int))\n",
    "np.sqrt(metrics.mean_squared_error(ytest, ypred))#Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
