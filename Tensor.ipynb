{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To load Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/james/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dataset_test = pd.read_csv('tcd-ml-1920-group-income-train.csv')\n",
    "dataset_pred = pd.read_csv('tcd-ml-1920-group-income-test.csv')\n",
    "store_data = dataset_pred.filter(['Instance'], axis=1)\n",
    "dataset_test.columns = dataset_test.columns.str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "dataset_pred.columns = dataset_pred.columns.str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "dataset_test.rename(columns={'Work_Experience_in_Current_Job_[years]': 'Work_Experience_in_Current_Job'}, inplace=True)\n",
    "dataset_pred.rename(columns={'Work_Experience_in_Current_Job_[years]': 'Work_Experience_in_Current_Job'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Instance Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test.drop('Instance', 1)\n",
    "dataset_pred = dataset_pred.drop('Instance', 1)\n",
    "dataset_test.drop(dataset_test.loc[dataset_test['Total_Yearly_Income_[EUR]']==2548791].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change null to rand value between std dev around mean. Alternate could be panda interpolate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/james/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "Column_Name_avg = dataset_test['Year_of_Record'].mean()\n",
    "Column_Name_std = dataset_test['Year_of_Record'].std()\n",
    "Column_Name_null_count = dataset_test['Year_of_Record'].isnull().sum()\n",
    "Column_Name_null_random_list = np.random.randint(Column_Name_avg - Column_Name_std, Column_Name_avg + Column_Name_std, size=Column_Name_null_count)\n",
    "dataset_test['Year_of_Record'][np.isnan(dataset_test['Year_of_Record'])] = Column_Name_null_random_list\n",
    "dataset_test['Year_of_Record'] = dataset_test['Year_of_Record'].astype(int)\n",
    "\n",
    "Column_Name_avg = dataset_pred['Year_of_Record'].mean()\n",
    "Column_Name_std = dataset_pred['Year_of_Record'].std()\n",
    "Column_Name_null_count = dataset_pred['Year_of_Record'].isnull().sum()\n",
    "Column_Name_null_random_list = np.random.randint(Column_Name_avg - Column_Name_std, Column_Name_avg + Column_Name_std, size=Column_Name_null_count)\n",
    "dataset_pred['Year_of_Record'][np.isnan(dataset_pred['Year_of_Record'])] = Column_Name_null_random_list\n",
    "dataset_pred['Year_of_Record'] = dataset_pred['Year_of_Record'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Gender Nan and other values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.Gender = dataset_test.Gender.replace(\"f\", \"female\")\n",
    "dataset_test[\"Gender\"].fillna(\"unknown\", inplace = True)\n",
    "dataset_test.Gender = dataset_test.Gender.replace(\"0\", \"unknown\")\n",
    "\n",
    "dataset_pred.Gender = dataset_pred.Gender.replace(\"f\", \"female\")\n",
    "dataset_pred[\"Gender\"].fillna(\"unknown\", inplace = True)\n",
    "dataset_pred.Gender = dataset_pred.Gender.replace(\"0\", \"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling University Degree Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.University_Degree = dataset_test.University_Degree.replace(\"0\", \"No\")\n",
    "dataset_test[\"University_Degree\"].fillna(\"No\", inplace = True)\n",
    "\n",
    "dataset_pred.University_Degree = dataset_pred.University_Degree.replace(\"0\", \"No\")\n",
    "dataset_pred[\"University_Degree\"].fillna(\"No\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Haircolor Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.Hair_Color = dataset_test.Hair_Color.replace(\"0\", \"Unknown\")\n",
    "dataset_test[\"Hair_Color\"].fillna(\"Unknown\", inplace = True)\n",
    "dataset_test = dataset_test.drop('Hair_Color', 1)\n",
    "\n",
    "dataset_pred.Hair_Color = dataset_pred.Hair_Color.replace(\"0\", \"Unknown\")\n",
    "dataset_pred[\"Hair_Color\"].fillna(\"Unknown\", inplace = True)\n",
    "dataset_pred = dataset_pred.drop('Hair_Color', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Profession Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test[\"Profession\"].fillna(\"Unknown\", inplace = True)\n",
    "dataset_pred[\"Profession\"].fillna(\"Unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Housing_Situation Corrupt Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test[\"Housing_Situation\"] = dataset_test[\"Housing_Situation\"].replace(0,'Unknown')\n",
    "dataset_test[\"Housing_Situation\"] = dataset_test[\"Housing_Situation\"].replace('0','Unknown')\n",
    "dataset_test[\"Housing_Situation\"] = dataset_test[\"Housing_Situation\"].replace('nA','Unknown')\n",
    "\n",
    "dataset_pred[\"Housing_Situation\"] = dataset_pred[\"Housing_Situation\"].replace(0,'Unknown')\n",
    "dataset_pred[\"Housing_Situation\"] = dataset_pred[\"Housing_Situation\"].replace('0','Unknown')\n",
    "dataset_pred[\"Housing_Situation\"] = dataset_pred[\"Housing_Situation\"].replace('nA','Unknown')\n",
    "\n",
    "\n",
    "dataset_test['Satisfation_with_employer'] = dataset_test['Satisfation_with_employer'].fillna(method='ffill')\n",
    "dataset_pred['Satisfation_with_employer'] = dataset_pred['Satisfation_with_employer'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert work Experience in current job to float and replace #NUM! to mean of their Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/james/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dataset_test.Work_Experience_in_Current_Job = dataset_test.Work_Experience_in_Current_Job.replace(\"#NUM!\", None)\n",
    "dataset_test['Work_Experience_in_Current_Job'] = dataset_test['Work_Experience_in_Current_Job'].astype(float)\n",
    "dataset_test.value = dataset_test.groupby('Age')['Work_Experience_in_Current_Job'].apply(lambda x: x.fillna(x.median()))\n",
    "dataset_test.value = dataset_test.Work_Experience_in_Current_Job.fillna(dataset_test.Work_Experience_in_Current_Job.median())\n",
    "\n",
    "dataset_pred.Work_Experience_in_Current_Job = dataset_pred.Work_Experience_in_Current_Job.replace(\"#NUM!\", None)\n",
    "dataset_pred['Work_Experience_in_Current_Job'] = dataset_pred['Work_Experience_in_Current_Job'].astype(float)\n",
    "dataset_pred.value = dataset_pred.groupby('Age')['Work_Experience_in_Current_Job'].apply(lambda x: x.fillna(x.median()))\n",
    "dataset_pred.value = dataset_pred.Work_Experience_in_Current_Job.fillna(dataset_pred.Work_Experience_in_Current_Job.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove EUR in Yearly_Income to make it numerical value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test['Yearly_Income_in_addition_to_Salary_e.g._Rental_Income'] = dataset_test['Yearly_Income_in_addition_to_Salary_e.g._Rental_Income'].replace('EUR', '', regex=True).astype(float)\n",
    "dataset_pred['Yearly_Income_in_addition_to_Salary_e.g._Rental_Income'] = dataset_pred['Yearly_Income_in_addition_to_Salary_e.g._Rental_Income'].replace('EUR', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset_test.dtypes[dataset_test.dtypes == 'object'].index.tolist():\n",
    "    feat_le = LabelEncoder()\n",
    "    train_list = dataset_test[col].unique()\n",
    "    dataset_test.loc[1201,col] = 'other'\n",
    "    test_list = dataset_pred[col].unique()\n",
    "    test_replace = list(set(test_list) - set(train_list))\n",
    "    dataset_pred[col] = dataset_pred[col].replace(test_replace, 'other')\n",
    "    feat_le.fit(dataset_test[col].unique().astype(str))\n",
    "    dataset_test[col] = feat_le.transform(dataset_test[col].astype(str))\n",
    "    dataset_pred[col] = feat_le.transform(dataset_pred[col].astype(str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function takes a string column name and returns a list\n",
    "### containing indices of dataframe that have outliers in that column\n",
    "### Refer: https://towardsdatascience.com/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlierByColumn(colname):\n",
    "    rows = dataset_test.shape[0]\n",
    "    col_std = np.std(dataset_test[colname])\n",
    "    col_mean = np.mean(dataset_test[colname])\n",
    "    anomaly_cut_off = col_std * 3\n",
    "    lower_limit  = col_mean - anomaly_cut_off \n",
    "    upper_limit = col_mean + anomaly_cut_off\n",
    "    anomalies_indices = []\n",
    "    for i in range(rows):\n",
    "        ele = dataset_test[colname][i]\n",
    "        if ele > upper_limit or ele < lower_limit:\n",
    "            anomalies_indices.append(i)\n",
    "    return anomalies_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = OutlierByColumn('Work_Experience_in_Current_Job')\n",
    "arr2 = OutlierByColumn('Age')\n",
    "arr3 = OutlierByColumn('Body_Height_[cm]')\n",
    "arr4 = OutlierByColumn('Yearly_Income_in_addition_to_Salary_e.g._Rental_Income')\n",
    "arr5 = OutlierByColumn('Size_of_City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Union of all lists \n",
    "union_index = np.union1d(arr1,arr2)\n",
    "union_index = np.union1d(union_index,arr3)\n",
    "union_index = np.union1d(union_index,arr4)\n",
    "union_index = np.union1d(union_index,arr5)\n",
    "len(union_index)\n",
    "dataset_test = dataset_test.drop(union_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "y = dataset_test['Total_Yearly_Income_[EUR]'].values\n",
    "dataset_test.drop('Total_Yearly_Income_[EUR]', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = dataset_test.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dataset_test = pd.DataFrame(x_scaled)\n",
    "X = dataset_test[dataset_test.columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.802083</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.561241</td>\n",
       "      <td>0.594834</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.254034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.076901</td>\n",
       "      <td>0.701845</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.369792</td>\n",
       "      <td>0.59375</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.488506</td>\n",
       "      <td>0.187905</td>\n",
       "      <td>0.727675</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.267165</td>\n",
       "      <td>0.847970</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.494792</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.661972</td>\n",
       "      <td>0.270115</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>0.596310</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1         2        3     4         5         6         7  \\\n",
       "0  0.506329  0.250  0.802083  0.46875  0.50  0.000000  0.295775  0.390805   \n",
       "1  0.898734  0.125  0.088542  0.62500  0.00  0.666667  0.478873  0.051724   \n",
       "2  0.050633  0.875  0.369792  0.59375  0.00  0.666667  0.422535  0.488506   \n",
       "3  0.721519  0.750  0.895833  0.68750  0.00  0.333333  0.549296  0.390805   \n",
       "4  0.506329  0.500  0.494792  0.71875  0.25  0.333333  0.661972  0.270115   \n",
       "\n",
       "          8         9    10   11        12        13  \n",
       "0  0.561241  0.594834  0.00  1.0  0.372881  0.254034  \n",
       "1  0.076901  0.701845  0.00  1.0  0.677966  0.000000  \n",
       "2  0.187905  0.727675  0.25  0.0  0.440678  0.000000  \n",
       "3  0.267165  0.847970  0.00  1.0  0.288136  0.000000  \n",
       "4  0.058381  0.596310  0.50  0.0  0.500000  0.000000  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 36387.11, 124103.01,   2421.29, ...,    823.84,   2709.58,\n",
       "       192709.31])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = dataset_test[dataset_test.columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(995179,)\n",
      "(995179, 14)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 80% of the data to the training set while 20% of the data to test set.\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(895661, 14) (99518, 14)\n",
      "(895661,) (99518,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred.drop('Total_Yearly_Income_[EUR]', axis=1, inplace=True)\n",
    "\n",
    "x = dataset_pred.values #returns a numpy array\n",
    "x_scaled = min_max_scaler.transform(x)\n",
    "dataset_pred = pd.DataFrame(x_scaled)\n",
    "\n",
    "X_pred = dataset_pred[dataset_pred.columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras import losses\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "losses = losses.mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024,kernel_initializer='RandomUniform', input_dim=14, activation= \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512,kernel_initializer='normal', activation= \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= losses , optimizer=sgd, metrics=[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 1024)              15360     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 540,673\n",
      "Trainable params: 540,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 716528 samples, validate on 179133 samples\n",
      "Epoch 1/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.6627 - mean_absolute_error: 22921.0227 - val_loss: 27.3157 - val_mean_absolute_error: 22579.3932\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 27.27337\n",
      "Epoch 2/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.6492 - mean_absolute_error: 22902.9688 - val_loss: 27.3286 - val_mean_absolute_error: 22310.9995\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 27.27337\n",
      "Epoch 3/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.5622 - mean_absolute_error: 22867.3176 - val_loss: 27.2038 - val_mean_absolute_error: 22634.7244\n",
      "\n",
      "Epoch 00003: val_loss improved from 27.27337 to 27.20384, saving model to weights-improvement-03-27.20.hdf5\n",
      "Epoch 4/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.5045 - mean_absolute_error: 22819.0516 - val_loss: 27.2892 - val_mean_absolute_error: 22735.1337\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 27.20384\n",
      "Epoch 5/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.4744 - mean_absolute_error: 22787.5281 - val_loss: 27.1369 - val_mean_absolute_error: 22311.7426\n",
      "\n",
      "Epoch 00005: val_loss improved from 27.20384 to 27.13695, saving model to weights-improvement-05-27.14.hdf5\n",
      "Epoch 6/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.4251 - mean_absolute_error: 22761.9970 - val_loss: 27.1175 - val_mean_absolute_error: 22401.1426\n",
      "\n",
      "Epoch 00006: val_loss improved from 27.13695 to 27.11752, saving model to weights-improvement-06-27.12.hdf5\n",
      "Epoch 7/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.3889 - mean_absolute_error: 22730.7837 - val_loss: 26.9863 - val_mean_absolute_error: 22360.2041\n",
      "\n",
      "Epoch 00007: val_loss improved from 27.11752 to 26.98628, saving model to weights-improvement-07-26.99.hdf5\n",
      "Epoch 8/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.3208 - mean_absolute_error: 22706.1756 - val_loss: 26.9780 - val_mean_absolute_error: 22378.6777\n",
      "\n",
      "Epoch 00008: val_loss improved from 26.98628 to 26.97798, saving model to weights-improvement-08-26.98.hdf5\n",
      "Epoch 9/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.2717 - mean_absolute_error: 22653.4936 - val_loss: 26.8976 - val_mean_absolute_error: 22136.9414\n",
      "\n",
      "Epoch 00009: val_loss improved from 26.97798 to 26.89761, saving model to weights-improvement-09-26.90.hdf5\n",
      "Epoch 10/50\n",
      "716528/716528 [==============================] - 10s 13us/step - loss: 28.2250 - mean_absolute_error: 22645.6339 - val_loss: 26.8399 - val_mean_absolute_error: 22076.0021\n",
      "\n",
      "Epoch 00010: val_loss improved from 26.89761 to 26.83990, saving model to weights-improvement-10-26.84.hdf5\n",
      "Epoch 11/50\n",
      "716528/716528 [==============================] - 10s 13us/step - loss: 29.2891 - mean_absolute_error: 22809.0094 - val_loss: 27.2011 - val_mean_absolute_error: 22479.1520\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 26.83990\n",
      "Epoch 12/50\n",
      "716528/716528 [==============================] - 10s 14us/step - loss: 28.4289 - mean_absolute_error: 22676.2349 - val_loss: 27.0900 - val_mean_absolute_error: 22557.9814\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 26.83990\n",
      "Epoch 13/50\n",
      "716528/716528 [==============================] - 21s 29us/step - loss: 28.2585 - mean_absolute_error: 22588.0786 - val_loss: 26.7192 - val_mean_absolute_error: 22290.8848\n",
      "\n",
      "Epoch 00013: val_loss improved from 26.83990 to 26.71924, saving model to weights-improvement-13-26.72.hdf5\n",
      "Epoch 14/50\n",
      "716528/716528 [==============================] - 11s 15us/step - loss: 28.1726 - mean_absolute_error: 22571.6003 - val_loss: 26.9817 - val_mean_absolute_error: 22538.7938\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 26.71924\n",
      "Epoch 15/50\n",
      "716528/716528 [==============================] - 7s 10us/step - loss: 28.1566 - mean_absolute_error: 22538.9950 - val_loss: 27.0423 - val_mean_absolute_error: 22250.7745\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 26.71924\n",
      "Epoch 16/50\n",
      "716528/716528 [==============================] - 7s 10us/step - loss: 28.0892 - mean_absolute_error: 22512.3432 - val_loss: 26.7974 - val_mean_absolute_error: 22625.5729\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 26.71924\n",
      "Epoch 17/50\n",
      "716528/716528 [==============================] - 12s 17us/step - loss: 27.9957 - mean_absolute_error: 22465.7381 - val_loss: 26.6156 - val_mean_absolute_error: 22231.2495\n",
      "\n",
      "Epoch 00017: val_loss improved from 26.71924 to 26.61559, saving model to weights-improvement-17-26.62.hdf5\n",
      "Epoch 18/50\n",
      "716528/716528 [==============================] - 34s 47us/step - loss: 27.9958 - mean_absolute_error: 22459.7783 - val_loss: 26.8944 - val_mean_absolute_error: 22281.7119\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 26.61559\n",
      "Epoch 19/50\n",
      "716528/716528 [==============================] - 47s 65us/step - loss: 27.9276 - mean_absolute_error: 22433.1019 - val_loss: 26.5293 - val_mean_absolute_error: 21963.9455\n",
      "\n",
      "Epoch 00019: val_loss improved from 26.61559 to 26.52931, saving model to weights-improvement-19-26.53.hdf5\n",
      "Epoch 20/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.9174 - mean_absolute_error: 22388.6216 - val_loss: 26.5586 - val_mean_absolute_error: 22287.4939\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 26.52931\n",
      "Epoch 21/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.9803 - mean_absolute_error: 22362.9423 - val_loss: 26.5267 - val_mean_absolute_error: 22283.3258\n",
      "\n",
      "Epoch 00021: val_loss improved from 26.52931 to 26.52668, saving model to weights-improvement-21-26.53.hdf5\n",
      "Epoch 22/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.9056 - mean_absolute_error: 22369.1714 - val_loss: 26.4083 - val_mean_absolute_error: 21794.5553\n",
      "\n",
      "Epoch 00022: val_loss improved from 26.52668 to 26.40835, saving model to weights-improvement-22-26.41.hdf5\n",
      "Epoch 23/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.8117 - mean_absolute_error: 22310.3309 - val_loss: 26.4328 - val_mean_absolute_error: 22074.0147\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.40835\n",
      "Epoch 24/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.8090 - mean_absolute_error: 22301.4361 - val_loss: 26.3774 - val_mean_absolute_error: 21861.6230\n",
      "\n",
      "Epoch 00024: val_loss improved from 26.40835 to 26.37745, saving model to weights-improvement-24-26.38.hdf5\n",
      "Epoch 25/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.7374 - mean_absolute_error: 22272.9960 - val_loss: 26.3281 - val_mean_absolute_error: 22055.5532\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.37745 to 26.32815, saving model to weights-improvement-25-26.33.hdf5\n",
      "Epoch 26/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.6747 - mean_absolute_error: 22238.5419 - val_loss: 27.2163 - val_mean_absolute_error: 22112.9649\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 26.32815\n",
      "Epoch 27/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.6430 - mean_absolute_error: 22230.1717 - val_loss: 26.3992 - val_mean_absolute_error: 22128.6843\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 26.32815\n",
      "Epoch 28/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.6424 - mean_absolute_error: 22198.7477 - val_loss: 26.2333 - val_mean_absolute_error: 22006.4902\n",
      "\n",
      "Epoch 00028: val_loss improved from 26.32815 to 26.23327, saving model to weights-improvement-28-26.23.hdf5\n",
      "Epoch 29/50\n",
      "716528/716528 [==============================] - 6s 9us/step - loss: 27.5543 - mean_absolute_error: 22136.1035 - val_loss: 26.5841 - val_mean_absolute_error: 22061.2167\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 26.23327\n",
      "Epoch 30/50\n",
      "716528/716528 [==============================] - 6s 9us/step - loss: 27.5462 - mean_absolute_error: 22149.1664 - val_loss: 26.0510 - val_mean_absolute_error: 21634.4049\n",
      "\n",
      "Epoch 00030: val_loss improved from 26.23327 to 26.05104, saving model to weights-improvement-30-26.05.hdf5\n",
      "Epoch 31/50\n",
      "716528/716528 [==============================] - 6s 9us/step - loss: 27.4754 - mean_absolute_error: 22107.7267 - val_loss: 26.0164 - val_mean_absolute_error: 21507.1965\n",
      "\n",
      "Epoch 00031: val_loss improved from 26.05104 to 26.01642, saving model to weights-improvement-31-26.02.hdf5\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.4907 - mean_absolute_error: 22074.2993 - val_loss: 26.0804 - val_mean_absolute_error: 21731.2735\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 26.01642\n",
      "Epoch 33/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.4603 - mean_absolute_error: 22081.8462 - val_loss: 26.0243 - val_mean_absolute_error: 21431.7235\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 26.01642\n",
      "Epoch 34/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.3684 - mean_absolute_error: 22049.9487 - val_loss: 25.9118 - val_mean_absolute_error: 21592.4147\n",
      "\n",
      "Epoch 00034: val_loss improved from 26.01642 to 25.91180, saving model to weights-improvement-34-25.91.hdf5\n",
      "Epoch 35/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.3223 - mean_absolute_error: 22002.3056 - val_loss: 25.9034 - val_mean_absolute_error: 21462.3933\n",
      "\n",
      "Epoch 00035: val_loss improved from 25.91180 to 25.90343, saving model to weights-improvement-35-25.90.hdf5\n",
      "Epoch 36/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.3076 - mean_absolute_error: 21978.3542 - val_loss: 25.8725 - val_mean_absolute_error: 21753.4271\n",
      "\n",
      "Epoch 00036: val_loss improved from 25.90343 to 25.87248, saving model to weights-improvement-36-25.87.hdf5\n",
      "Epoch 37/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.3148 - mean_absolute_error: 21957.1179 - val_loss: 25.8396 - val_mean_absolute_error: 21411.1926\n",
      "\n",
      "Epoch 00037: val_loss improved from 25.87248 to 25.83958, saving model to weights-improvement-37-25.84.hdf5\n",
      "Epoch 38/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.2802 - mean_absolute_error: 21947.2663 - val_loss: 25.8160 - val_mean_absolute_error: 21750.5566\n",
      "\n",
      "Epoch 00038: val_loss improved from 25.83958 to 25.81603, saving model to weights-improvement-38-25.82.hdf5\n",
      "Epoch 39/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.2281 - mean_absolute_error: 21935.6056 - val_loss: 25.7362 - val_mean_absolute_error: 21639.3647\n",
      "\n",
      "Epoch 00039: val_loss improved from 25.81603 to 25.73616, saving model to weights-improvement-39-25.74.hdf5\n",
      "Epoch 40/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.2486 - mean_absolute_error: 21903.3637 - val_loss: 25.7292 - val_mean_absolute_error: 21523.6938\n",
      "\n",
      "Epoch 00040: val_loss improved from 25.73616 to 25.72920, saving model to weights-improvement-40-25.73.hdf5\n",
      "Epoch 41/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.2070 - mean_absolute_error: 21882.6890 - val_loss: 25.7283 - val_mean_absolute_error: 21599.4001\n",
      "\n",
      "Epoch 00041: val_loss improved from 25.72920 to 25.72830, saving model to weights-improvement-41-25.73.hdf5\n",
      "Epoch 42/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.0801 - mean_absolute_error: 21826.8495 - val_loss: 25.7106 - val_mean_absolute_error: 21577.2841\n",
      "\n",
      "Epoch 00042: val_loss improved from 25.72830 to 25.71064, saving model to weights-improvement-42-25.71.hdf5\n",
      "Epoch 43/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.0621 - mean_absolute_error: 21837.0526 - val_loss: 25.6008 - val_mean_absolute_error: 21231.0325\n",
      "\n",
      "Epoch 00043: val_loss improved from 25.71064 to 25.60081, saving model to weights-improvement-43-25.60.hdf5\n",
      "Epoch 44/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 27.0318 - mean_absolute_error: 21807.8051 - val_loss: 25.6207 - val_mean_absolute_error: 21530.1602\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 25.60081\n",
      "Epoch 45/50\n",
      "716528/716528 [==============================] - 6s 9us/step - loss: 27.0100 - mean_absolute_error: 21790.6595 - val_loss: 25.6856 - val_mean_absolute_error: 21606.6475\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 25.60081\n",
      "Epoch 46/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 26.9781 - mean_absolute_error: 21756.0275 - val_loss: 25.5226 - val_mean_absolute_error: 21775.0196\n",
      "\n",
      "Epoch 00046: val_loss improved from 25.60081 to 25.52257, saving model to weights-improvement-46-25.52.hdf5\n",
      "Epoch 47/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 26.9766 - mean_absolute_error: 21753.1949 - val_loss: 25.5332 - val_mean_absolute_error: 21357.1200\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 25.52257\n",
      "Epoch 48/50\n",
      "716528/716528 [==============================] - 7s 10us/step - loss: 26.8836 - mean_absolute_error: 21710.8634 - val_loss: 25.5037 - val_mean_absolute_error: 21335.1708\n",
      "\n",
      "Epoch 00048: val_loss improved from 25.52257 to 25.50368, saving model to weights-improvement-48-25.50.hdf5\n",
      "Epoch 49/50\n",
      "716528/716528 [==============================] - 7s 9us/step - loss: 26.9614 - mean_absolute_error: 21703.3046 - val_loss: 25.4508 - val_mean_absolute_error: 21288.4772\n",
      "\n",
      "Epoch 00049: val_loss improved from 25.50368 to 25.45076, saving model to weights-improvement-49-25.45.hdf5\n",
      "Epoch 50/50\n",
      "716528/716528 [==============================] - 7s 10us/step - loss: 26.8524 - mean_absolute_error: 21655.1952 - val_loss: 25.6734 - val_mean_absolute_error: 21516.4939\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 25.45076\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=1024, epochs=50,\n",
    "          validation_split=0.2,shuffle=True,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99518/99518 [==============================] - 6s 63us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21589.146241465896"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('/home/james/Documents/MachineLearning/ML-GroupAssignment/weights-improvement-188-23.06.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 807659 samples, validate on 89740 samples\n",
      "Epoch 1/50\n",
      "807659/807659 [==============================] - 3s 4us/step - loss: 26.0402 - mean_absolute_error: 19042.0743 - val_loss: 23.2465 - val_mean_absolute_error: 18200.0379\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 22.37025\n",
      "Epoch 2/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.7174 - mean_absolute_error: 18944.0640 - val_loss: 23.7485 - val_mean_absolute_error: 18533.6965\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 22.37025\n",
      "Epoch 3/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.6983 - mean_absolute_error: 18947.1850 - val_loss: 23.7833 - val_mean_absolute_error: 18539.0371\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 22.37025\n",
      "Epoch 4/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.6915 - mean_absolute_error: 18980.7236 - val_loss: 23.6079 - val_mean_absolute_error: 18304.7194\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 22.37025\n",
      "Epoch 5/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.6738 - mean_absolute_error: 18945.2303 - val_loss: 23.3481 - val_mean_absolute_error: 18313.6947\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 22.37025\n",
      "Epoch 6/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.6449 - mean_absolute_error: 18909.8275 - val_loss: 23.3364 - val_mean_absolute_error: 18237.7937\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 22.37025\n",
      "Epoch 7/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.6065 - mean_absolute_error: 18954.9793 - val_loss: 23.4666 - val_mean_absolute_error: 18293.0894\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 22.37025\n",
      "Epoch 8/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.6080 - mean_absolute_error: 18933.6604 - val_loss: 23.3990 - val_mean_absolute_error: 18257.4796\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 22.37025\n",
      "Epoch 9/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5908 - mean_absolute_error: 18937.9637 - val_loss: 23.7331 - val_mean_absolute_error: 18496.6756\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 22.37025\n",
      "Epoch 10/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5833 - mean_absolute_error: 18928.4796 - val_loss: 23.5912 - val_mean_absolute_error: 18312.6781\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 22.37025\n",
      "Epoch 11/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5584 - mean_absolute_error: 18925.6999 - val_loss: 23.5164 - val_mean_absolute_error: 18463.8481\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 22.37025\n",
      "Epoch 12/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5440 - mean_absolute_error: 18920.5976 - val_loss: 23.7520 - val_mean_absolute_error: 18525.1972\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 22.37025\n",
      "Epoch 13/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5775 - mean_absolute_error: 18920.2444 - val_loss: 23.6855 - val_mean_absolute_error: 18478.2027\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 22.37025\n",
      "Epoch 14/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5101 - mean_absolute_error: 18886.9229 - val_loss: 23.5866 - val_mean_absolute_error: 18290.4482\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 22.37025\n",
      "Epoch 15/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5017 - mean_absolute_error: 18920.2337 - val_loss: 23.6598 - val_mean_absolute_error: 18515.6456\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 22.37025\n",
      "Epoch 16/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5267 - mean_absolute_error: 18886.2284 - val_loss: 23.5460 - val_mean_absolute_error: 18488.2701\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 22.37025\n",
      "Epoch 17/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5288 - mean_absolute_error: 18901.3641 - val_loss: 23.3737 - val_mean_absolute_error: 18355.8187\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 22.37025\n",
      "Epoch 18/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5019 - mean_absolute_error: 18897.6606 - val_loss: 23.5608 - val_mean_absolute_error: 18385.1322\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 22.37025\n",
      "Epoch 19/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5755 - mean_absolute_error: 18890.0720 - val_loss: 23.4239 - val_mean_absolute_error: 18294.5399\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 22.37025\n",
      "Epoch 20/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4984 - mean_absolute_error: 18851.5115 - val_loss: 23.3596 - val_mean_absolute_error: 18340.1646\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 22.37025\n",
      "Epoch 21/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.5145 - mean_absolute_error: 18875.2361 - val_loss: 23.6765 - val_mean_absolute_error: 18499.4552\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 22.37025\n",
      "Epoch 22/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4890 - mean_absolute_error: 18911.9268 - val_loss: 23.6152 - val_mean_absolute_error: 18431.8786\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 22.37025\n",
      "Epoch 23/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4563 - mean_absolute_error: 18881.5882 - val_loss: 23.4698 - val_mean_absolute_error: 18392.2903\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 22.37025\n",
      "Epoch 24/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4875 - mean_absolute_error: 18889.2809 - val_loss: 23.7645 - val_mean_absolute_error: 18449.5820\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 22.37025\n",
      "Epoch 25/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4511 - mean_absolute_error: 18857.9852 - val_loss: 24.0546 - val_mean_absolute_error: 18599.9387\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 22.37025\n",
      "Epoch 26/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4890 - mean_absolute_error: 18891.8731 - val_loss: 23.5165 - val_mean_absolute_error: 18478.1047\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 22.37025\n",
      "Epoch 27/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4350 - mean_absolute_error: 18842.9078 - val_loss: 23.6948 - val_mean_absolute_error: 18495.4401\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 22.37025\n",
      "Epoch 28/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4407 - mean_absolute_error: 18831.2973 - val_loss: 23.7550 - val_mean_absolute_error: 18380.3111\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 22.37025\n",
      "Epoch 29/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4176 - mean_absolute_error: 18857.9072 - val_loss: 23.8843 - val_mean_absolute_error: 18576.7162\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 22.37025\n",
      "Epoch 30/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4212 - mean_absolute_error: 18874.0584 - val_loss: 23.9057 - val_mean_absolute_error: 18594.1292\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 22.37025\n",
      "Epoch 31/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3952 - mean_absolute_error: 18825.1051 - val_loss: 23.5998 - val_mean_absolute_error: 18331.1582\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22.37025\n",
      "Epoch 32/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3924 - mean_absolute_error: 18846.3215 - val_loss: 23.9006 - val_mean_absolute_error: 18536.4701\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 22.37025\n",
      "Epoch 33/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3633 - mean_absolute_error: 18825.2027 - val_loss: 24.0257 - val_mean_absolute_error: 18620.2276\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22.37025\n",
      "Epoch 34/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3611 - mean_absolute_error: 18833.9069 - val_loss: 23.9787 - val_mean_absolute_error: 18453.0510\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22.37025\n",
      "Epoch 35/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3628 - mean_absolute_error: 18818.1199 - val_loss: 24.0267 - val_mean_absolute_error: 18658.6173\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22.37025\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3914 - mean_absolute_error: 18862.2110 - val_loss: 24.1459 - val_mean_absolute_error: 18807.5961\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22.37025\n",
      "Epoch 37/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3523 - mean_absolute_error: 18826.0674 - val_loss: 23.7801 - val_mean_absolute_error: 18593.8260\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 22.37025\n",
      "Epoch 38/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3756 - mean_absolute_error: 18836.6858 - val_loss: 24.4340 - val_mean_absolute_error: 18888.4062\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.37025\n",
      "Epoch 39/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3729 - mean_absolute_error: 18833.7592 - val_loss: 24.1205 - val_mean_absolute_error: 18559.6316\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 22.37025\n",
      "Epoch 40/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3307 - mean_absolute_error: 18807.5731 - val_loss: 23.9199 - val_mean_absolute_error: 18615.6030\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.37025\n",
      "Epoch 41/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3106 - mean_absolute_error: 18783.1960 - val_loss: 23.9480 - val_mean_absolute_error: 18652.9919\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.37025\n",
      "Epoch 42/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3356 - mean_absolute_error: 18812.5105 - val_loss: 23.9415 - val_mean_absolute_error: 18564.5673\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 22.37025\n",
      "Epoch 43/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3330 - mean_absolute_error: 18802.6425 - val_loss: 23.9125 - val_mean_absolute_error: 18601.1930\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 22.37025\n",
      "Epoch 44/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4319 - mean_absolute_error: 18832.5193 - val_loss: 23.9917 - val_mean_absolute_error: 18639.8829\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 22.37025\n",
      "Epoch 45/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.4523 - mean_absolute_error: 18822.5960 - val_loss: 23.9862 - val_mean_absolute_error: 18692.0405\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 22.37025\n",
      "Epoch 46/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3104 - mean_absolute_error: 18766.7967 - val_loss: 23.9504 - val_mean_absolute_error: 18551.1115\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 22.37025\n",
      "Epoch 47/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.2951 - mean_absolute_error: 18790.2278 - val_loss: 23.8744 - val_mean_absolute_error: 18684.5408\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 22.37025\n",
      "Epoch 48/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.2821 - mean_absolute_error: 18781.2116 - val_loss: 23.7541 - val_mean_absolute_error: 18498.7273\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 22.37025\n",
      "Epoch 49/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.3057 - mean_absolute_error: 18816.1065 - val_loss: 23.8029 - val_mean_absolute_error: 18657.9041\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 22.37025\n",
      "Epoch 50/50\n",
      "807659/807659 [==============================] - 2s 3us/step - loss: 25.2816 - mean_absolute_error: 18796.5742 - val_loss: 23.9768 - val_mean_absolute_error: 18644.9227\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 22.37025\n"
     ]
    }
   ],
   "source": [
    "hist = model2.fit(X_train, Y_train,\n",
    "          batch_size=16384, epochs=50,\n",
    "          validation_split=0.1,shuffle=True,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data['Total Yearly Income [EUR]'] = y_pred\n",
    "store_data.to_csv('output6.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
